{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onKYHAy6hwFT"
   },
   "source": [
    "<img src=\"images/images_0.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuUJICnTcN3d"
   },
   "source": [
    "# Library Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bu0Vtx7KroQf"
   },
   "outputs": [],
   "source": [
    "!pip3 install langchain langchain-community transformers sentence-transformers faiss-gpu pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EsmA791uGMh"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForQuestionAnswering\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import HuggingFaceHub\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/images_1.png\" width=\"400\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/images_2.png\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inNQW-1tuNvH"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/content/Statistics Note for Data science .pdf\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                                          chunk_size=1000,\n",
    "                                          chunk_overlap=50,\n",
    "                                          separators=[\"\\n\\n\\n\", \"\\n\\n\", \";\",\".\",\" \"]\n",
    "                                              )\n",
    "\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/images_3.png\" width=\"800\" height='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vf_T79_2TYCL",
    "outputId": "9452f2c7-0852-4781-cd9f-2c7c5b0f61eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "emb_kwargs={'model_name' :\"sentence-transformers/all-MiniLM-l6-v2\",\n",
    "'model_kwargs':{'device':'cuda'},\n",
    "'encode_kwargs':{'normalize_embeddings': False}}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(**emb_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/images_4.png\" width=\"600\" height='300'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcOEEuYnSqxn"
   },
   "outputs": [],
   "source": [
    "#vector store\n",
    "\n",
    "db = FAISS.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cos(\\theta)=\\frac{A.B}{||A|| \\ ||B||}=\\frac{\\sum_{i=1}^n  A_i B_i}{\\sqrt{\\sum_{i=1}^n  A_i^2} \\sqrt{\\sum_{i=1}^n B_i^2}}$$\n",
    "\n",
    "<img src=\"images/images_5.png\" width=\"400\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJmkv9OLTunP",
    "outputId": "fb97ae30-91fb-4896-d9cb-d909e258bffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. Pearson correlation coefficient: - The Pearson correlation coefficient (r) is the \n",
      "most common way of measuring a linear correlation. It is a number between –1 and 1 \n",
      "that measures the strength and direction of the relationship between two variables. \n",
      " \n",
      "Pearson \n",
      "correlation \n",
      "coefficient (r) \n",
      "Correlation type Interpretation Example \n",
      "Between 0 and 1 Positive correlation When one variable \n",
      "changes, the other \n",
      "variable changes in \n",
      "the same direction. \n",
      "Baby length & weight: \n",
      "The longer the baby, the \n",
      "heavier their weight. \n",
      "0 No correlation There is no \n",
      "relationship between \n",
      "the variables. \n",
      "Car price & width of \n",
      "windshield wipers: \n",
      "The price of a car is not \n",
      "related to the width of its \n",
      "windshield wipers. \n",
      "Between \n",
      "0 and –1 \n",
      "Negative \n",
      "correlation \n",
      "When one variable \n",
      "changes, the other \n",
      "variable changes in \n",
      "the opposite direction. \n",
      "Elevation & air pressure: \n",
      "The higher the elevation, \n",
      "the lower the air pressure. \n",
      " \n",
      " \n",
      " \n",
      "where \n",
      "• cov  is the covariance \n",
      "• σx is the standard deviation of X \n",
      "• σy is the standard deviation of Y \n",
      " \n",
      " \n",
      "B. Spearman's rank correlation coefficient:- A correlation can easily be drawn as \n",
      "a scatter graph, but the most precise way to compare several pairs of data is to use a \n",
      "statistical test - this establishes whether the correlation is really significant or if it \n",
      "could have been the result of chance alone\n"
     ]
    }
   ],
   "source": [
    "question = \"what Pearson correlation coefficient?\"\n",
    "searchDocs = db.similarity_search(question)\n",
    "print(searchDocs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/images_6.png\" width=\"600\" height='300'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgQzbCK62qQs",
    "outputId": "8ae9698b-47d4-4451-ca4f-ff6b2dcbc8d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: MBZUAI/LaMini-T5-738M\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
    "print(f\"Checkpoint path: {checkpoint}\")  # Add this line for debugging\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model = base_model,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = 256,\n",
    "        do_sample = True,\n",
    "        temperature = 0.3,\n",
    "        top_p= 0.95,\n",
    "    )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7LQlX0kQqBq"
   },
   "outputs": [],
   "source": [
    "\n",
    "#llm = HuggingFaceHub(repo_id=\"MBZUAI/LaMini-T5-738M\", model_kwargs={\"temperature\":0.5, \"max_length\": 512})\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=db.as_retriever(),\n",
    "        memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQtJY-yXZtmG",
    "outputId": "13324706-ab4a-4fe0-90c0-644220b7e6c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'what Pearson correlation coefficient?',\n",
       " 'chat_history': [HumanMessage(content='what Pearson correlation coefficient?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The Pearson correlation coefficient is a number between -1 and 1.', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'The Pearson correlation coefficient is a number between -1 and 1.'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain({\"question\": \"what Pearson correlation coefficient?\"})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
